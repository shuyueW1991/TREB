{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data prep: zero_one_normalization and its regeneration with mins, max.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def zero_one_normalization(df):\n",
    "    \"\"\"\n",
    "    Applies 0-1 median normalization to each column of a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: A pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with normalized values, a Series of minimum values, and a Series of maximum values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate median, minimum, and maximum for each column\n",
    "    mins = df.min()\n",
    "    maxs = df.max()\n",
    "\n",
    "    # Apply 0-1 normalization\n",
    "    normalized_df = (df - mins) / (maxs - mins)\n",
    "\n",
    "    return normalized_df, mins, maxs\n",
    "\n",
    "def regenerate_data(normalized_df, mins, maxs):\n",
    "    original_df = (normalized_df * (maxs - mins)) + mins\n",
    "    return original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv =  'cal_dataframe.csv'\n",
    "normalized_csv =  \"cal_dataframe_normalized.csv\"\n",
    "mins_csv = \"cal_dataframe_mins.csv\"\n",
    "maxs_csv = \"cal_dataframe_maxs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The raw data looks like\n",
    "df = pd.read_csv(input_csv)  \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data, then round it\n",
    "normalized_df, mins, maxs = zero_one_normalization(df)\n",
    "normalized_df = normalized_df.round(decimals=4)\n",
    "\n",
    "normalized_df = normalized_df.sample(frac=1)\n",
    "\n",
    "\n",
    "# Save the normalized data to a new CSV file\n",
    "normalized_df.to_csv(normalized_csv, index=False)\n",
    "mins.to_csv(mins_csv, index=False)\n",
    "maxs.to_csv(maxs_csv, index=False)\n",
    "\n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerate the original data, as verification\n",
    "original_df = regenerate_data(normalized_df, mins, maxs)\n",
    "original_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def csv_to_sentences(input_file, output_file):\n",
    "\n",
    "    with open(input_file, 'r') as csvfile, open(output_file, 'w', newline='') as outfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        writer = csv.writer(outfile)\n",
    "\n",
    "        for row in reader:\n",
    "            # sentence = \", \".join(f\"Column {index}: {round(float(value),4)}\" for index, value in enumerate(row.values())) + \" \"\n",
    "            sentence = \", \".join(f\"Column {index}: {str(round(float(value),4)).ljust(6,'0')}\" for index, value in enumerate(row.values())) + \" \"\n",
    "            writer.writerow([sentence])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = \"cal_dataframe_result.csv\"\n",
    "csv_to_sentences(normalized_csv, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_into_list(filename):\n",
    "    \"\"\"\n",
    "    Reads a CSV file into a list, where each row (excluding the header) is an element.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the rows of the CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            # print('row', row)\n",
    "            data.append(row[0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building a 4-digit-number friendly tokenizer based on <bert-based-uncased> model.\n",
    "https://huggingface.co/learn/nlp-course/chapter6/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_txt  = \"4_digit_integer_others.txt\"\n",
    "with open(output_txt, \"w\") as f:\n",
    "    for i in range(1000,int(1e4)):\n",
    "        f.write(str(i) + \"\\n\")\n",
    "    \n",
    "    for i in range(int(1e3)):\n",
    "        f.write(str(i).zfill(4)  + \"\\n\")\n",
    "        \n",
    "    f.write('Column' + '\\n')\n",
    "    f.write(':' + '\\n')\n",
    "    f.write(',' + '\\n')\n",
    "    f.write(' ' + '\\n')\n",
    "    f.write('.' + '\\n')\n",
    "    # f.write('[UNK]' + '\\n')\n",
    "\n",
    "data_list = read_csv_into_list(output_txt)\n",
    "\n",
    "def get_training_corpus():\n",
    "    for start_idx in range(0, len(data_list), 500):\n",
    "        samples = data_list[start_idx : start_idx + 500]\n",
    "        # print('samples', samples)\n",
    "        yield samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer\n",
    "# # old_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# old_tokenizer = BertTokenizer.from_pretrained(\"../pretrainedModels_and_archivedFiles_and_stuffs/bert-base-uncased\")\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "# old_tokenizer = AutoTokenizer.from_pretrained(\"../pretrainedModels_and_archivedFiles_and_stuffs/bert-base-uncased\")\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"./bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = data_list[177]\n",
    "# example = \"This is a text with 123.1279 numbers.\"\n",
    "# example = \"Column 0: 3.2705, Column 1: 52.0, Column 2: 4.7725, Column 3: 1.0245, Column 4: 1504.0 \"\n",
    "# example = \"Column 0: 3.2705, Column 1: 52.0, Column 2: 4.7799, Column 3: 1.0245, Column 4: 1504.0 \"\n",
    "\n",
    "example = pd.read_csv(\"cal_dataframe_result.csv\",header=None).iloc[[2,3,4,5,6,7,8]]\n",
    "# example = pd.read_csv(\"cal_dataframe_result.csv\",header=None).iloc[[2,3],:]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = old_tokenizer.tokenize(example)\n",
    "# tokens\n",
    "\n",
    "for index, row in example.iterrows():\n",
    "    tk = old_tokenizer.tokenize(row[0])\n",
    "    print(tk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", which is bad. I need numbers clearly cut from text. Let us train a new tokenizer.\n",
    "\n",
    "some other references that i dont fllow.\n",
    "https://github.com/huggingface/tokenizers/blob/main/bindings/python/examples/train_bert_wordpiece.py\n",
    "https://discuss.huggingface.co/t/dealing-with-decimal-and-fractions/23377/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab_file = \"4_digit_integer_others.txt\"\n",
    "additional_tokens = ['.', ' ', 'Column', ':']\n",
    "\n",
    "with open(vocab_file, 'r') as f:\n",
    "    vocab_list = [line.strip() for line in f]\n",
    "\n",
    "\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(\"./bert-base-uncased\", vocab_file=vocab_file)\n",
    "######\n",
    "# https://stackoverflow.com/questions/60914793/argument-never-split-not-working-on-bert-tokenizer\n",
    "######\n",
    "\n",
    "new_tokenizer.add_tokens((vocab_list))\n",
    "new_tokenizer.never_split = vocab_list\n",
    "\n",
    "for index, row in example.iterrows():\n",
    "    tk = new_tokenizer.tokenize(row[0])\n",
    "    print(tk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer.save_pretrained(\"the_tokenizer_the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
