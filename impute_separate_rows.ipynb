{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def csv_to_sentences_inference(input_file, output_file):\n",
    "    with open(input_file, 'r') as csvfile, open(output_file, 'w', newline='') as outfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        writer = csv.writer(outfile)\n",
    "\n",
    "        for row in reader:\n",
    "            # sentence = \", \".join(f\"Column {index}: {value}\" for index, value in enumerate(row.values())) + \" \"\n",
    "            sentence = \", \".join(f\"Column {index}: {value}\" for index, value in enumerate(row.values())) + \" \"\n",
    "            writer.writerow([sentence])\n",
    "\n",
    "\n",
    "def read_csv_into_list_inference(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            # print('row', row)\n",
    "            data.append(row[0])\n",
    "            # data.append(row)\n",
    "    return data\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "# fill = pipeline('fill-mask', model='the_TREB_model', tokenizer='the_tokenizer', device=0)\n",
    "fill = pipeline('fill-mask', model='the_TREB_model_the_ultimate', tokenizer='the_tokenizer_the', device=0)\n",
    "fill\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'cal_dataframe_normalized_emptied_0.csv'\n",
    "filename_result = filename.replace('.csv', '_result.csv')\n",
    "csv_to_sentences_inference(filename, filename_result)\n",
    "empty_column_indices = [0]  \n",
    "timu = read_csv_into_list_inference(filename_result)\n",
    "timu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_data = []\n",
    "\n",
    "for tim in timu:\n",
    "    res = fill(tim) \n",
    "    print('res', res)\n",
    "    # print('res', res[0])\n",
    "    # print('res', res[0]['token_str'])\n",
    "    mask_position = [m.start() for m in re.finditer('[MASK]', tim)]\n",
    "    # print('mask_position', mask_position)\n",
    "\n",
    "    # Replace each 'MASK' with the corresponding string\n",
    "    modified_text = \"\"\n",
    "    modified_text += tim[0:mask_position[1]] + res[0]['token_str']\n",
    "    modified_text += tim[mask_position[-1]+1:]\n",
    "    text_data.append(modified_text.replace('\\n', ''))\n",
    "\n",
    "with open(filename.replace('.csv', '') + '_imputed.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read a row from the txt file and parse it into a dictionary\n",
    "def parse_row(row):\n",
    "    data = {}\n",
    "    for column_value in row.split(', '):\n",
    "        key, value = column_value.split(': ')\n",
    "        data[key] = float(value)  # Convert values to float\n",
    "    return data\n",
    "\n",
    "# Read the txt file and create a list of dictionaries\n",
    "with open(filename.replace('.csv', '') + '_imputed.txt', 'r') as f:\n",
    "# with open('test.txt', 'r') as f:\n",
    "    datatxt = [parse_row(row) for row in f]\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "dftxt = pd.DataFrame(datatxt)\n",
    "# dftxt\n",
    "\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "dftxt.to_csv(filename.replace('.csv', '') + '_imputed.csv', index=False)  # Save without index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_data(normalized_df, mins, maxs):\n",
    "    \"\"\"\n",
    "    Regenerates the original data from the normalized data, minimum, and maximum values.\n",
    "\n",
    "    Args:\n",
    "        normalized_df: A pandas DataFrame with normalized values.\n",
    "        mins: A pandas Series containing the minimum values of each column.\n",
    "        maxs: A pandas Series containing the maximum values of each column.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with the original data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Regenerate original data\n",
    "    original_df = (normalized_df * (maxs - mins)) + mins\n",
    "\n",
    "    return original_df\n",
    "\n",
    "\n",
    "\n",
    "mins = pd.read_csv('cal_dataframe_mins.csv')\n",
    "maxs = pd.read_csv('cal_dataframe_maxs.csv')\n",
    "result_0 = regenerate_data(dftxt, mins, maxs)\n",
    "result_0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
